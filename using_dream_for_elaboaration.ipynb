{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset('cos_e', 'v1.11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "# Example to load from the directory named \"formatted_dataset\"\n",
    "loaded_dataset = load_from_disk('./formatted_dataset')\n",
    "\n",
    "# Verify the loaded dataset\n",
    "print(loaded_dataset['train'][1]['generated_output'])\n",
    "print(loaded_dataset['validation'][1]['generated_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove \\n$answer$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_answer_prefix(example):\n",
    "    if 'generated_output' in example:\n",
    "        example['generated_output'] = [output.replace(\"$answer$ =\", \"\").strip() for output in example['generated_output']]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Apply the function to the dataset\n",
    "transformed_dataset = loaded_dataset.map(remove_answer_prefix, batched=True)\n",
    "\n",
    "# Verify the transformation\n",
    "print(transformed_dataset['train'][0]['generated_output'])\n",
    "print(transformed_dataset['validation'][0]['generated_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([transformed_dataset['validation'][0]['generated_output']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the transformation\n",
    "print(transformed_dataset['train'][1]['generated_output'])\n",
    "print(transformed_dataset['validation'][1]['generated_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"  # Use \"t5-base\" or \"t5-large\" if resources allow\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "must be change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the dataset\n",
    "# def preprocess_function(examples):\n",
    "#     inputs = [\"question: \" + q + \" answer: \" + \" \".join(choices) for q, choices in zip(examples['question'], examples['choices'])]\n",
    "#     targets = [\"answer: \" + answer + \" explanation: \" + explanation for answer, explanation in zip(examples['answer'], examples['abstractive_explanation'])]\n",
    "#     model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding='max_length')\n",
    "#     labels = tokenizer(targets, max_length=256, truncation=True, padding='max_length')\n",
    "#     model_inputs['labels'] = labels['input_ids']\n",
    "#     return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    # Check if 'generated_output' exists in the examples\n",
    "    if 'generated_output' not in examples:\n",
    "        raise ValueError(\"The 'generated_output' field is missing in the dataset examples.\")\n",
    "\n",
    "    # Concatenate each generated_output to the end of the input\n",
    "    inputs = [\"question: \" + q + \" answer: \" + \" \".join(choices) + \" \" + generated_output\n",
    "              for q, choices, generated_output in zip(examples['question'], examples['choices'], examples['generated_output'])]\n",
    "    \n",
    "    targets = [\"answer: \" + answer + \" explanation: \" + explanation \n",
    "               for answer, explanation in zip(examples['answer'], examples['abstractive_explanation'])]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(targets, max_length=256, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = transformed_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few examples to verify the preprocessing\n",
    "print(encoded_dataset['train'][0]['input_ids'])\n",
    "print(encoded_dataset['train'][0]['labels'])\n",
    "print(encoded_dataset['validation'][0]['input_ids'])\n",
    "print(encoded_dataset['validation'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode the encoded inputs and labels for validation\n",
    "def decode_example(example):\n",
    "    input_ids = example['input_ids']\n",
    "    labels = example['labels']\n",
    "    \n",
    "    decoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    decoded_label = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    return decoded_input, decoded_label\n",
    "\n",
    "# Decode and print a few examples from the training set\n",
    "for i in range(3):\n",
    "    decoded_input, decoded_label = decode_example(encoded_dataset['train'][i])\n",
    "    print(f\"Example {i + 1} - Decoded Input: {decoded_input}\")\n",
    "    print(f\"Example {i + 1} - Decoded Label: {decoded_label}\\n\")\n",
    "\n",
    "# Decode and print a few examples from the validation set\n",
    "for i in range(3):\n",
    "    decoded_input, decoded_label = decode_example(encoded_dataset['validation'][i])\n",
    "    print(f\"Example {i + 1} - Decoded Input: {decoded_input}\")\n",
    "    print(f\"Example {i + 1} - Decoded Label: {decoded_label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Convert dataset to PyTorch tensors\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(encoded_dataset['train'], batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(encoded_dataset['validation'], batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_loop(model, loader, optimizer, accumulation_steps=2):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(loader, desc='Training:')):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "        loss = outputs.loss / accumulation_steps  # normalize loss\n",
    "\n",
    "        batch_loss_value = loss.item() * accumulation_steps  # convert to original loss value for logging\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:  # update weights every accumulation_steps mini-batches\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # reset gradients\n",
    "\n",
    "        batch_losses.append(batch_loss_value)\n",
    "\n",
    "    # Update remaining gradients if the number of batches is not a multiple of accumulation_steps\n",
    "    if len(loader) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    return {'train_loss': loss_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sentences(list_of_lists):\n",
    "    sentences = [' '.join(inner_list) for inner_list in list_of_lists]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score\n",
    "from datasets import load_metric\n",
    "def validate_loop(model, loader):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    accuracy_preds = []\n",
    "    accuracy_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Validation:'):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Generate predictions\n",
    "            predictions = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512)\n",
    "\n",
    "            # Decode predictions and labels\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Print decoded predictions and labels for debugging\n",
    "            # print(\"Decoded predictions:\")\n",
    "            # for pred in decoded_preds:\n",
    "            #     print(f\"'{pred}'\")\n",
    "            # print(\"Decoded labels:\")\n",
    "            # for label in decoded_labels:\n",
    "            #     print(f\"'{label}'\")\n",
    "\n",
    "            # # Extract the answers from decoded predictions and labels\n",
    "            # extracted_preds = [pred.strip().split('.')[0] for pred in decoded_preds if pred.strip()]\n",
    "            # extracted_labels = [label.strip().split('.')[0] for label in decoded_labels if label.strip()]\n",
    "            # Extract the answers from decoded predictions and labels\n",
    "            extracted_preds = [pred.split('answer: ')[1].split(' ')[0] for pred in decoded_preds if 'answer: ' in pred]\n",
    "            extracted_labels = [label.split('answer: ')[1].split(' ')[0] for label in decoded_labels if 'answer: ' in label]\n",
    "            \n",
    "            explanations_preds = [pred.split('explanation: ')[1].split(' ') for pred in decoded_preds if 'explanation: ' in pred]\n",
    "            explanations_labels = [label.split('explanation: ')[1].split(' ') for label in decoded_labels if 'explanation: ' in label]\n",
    "            # print(convert_to_sentences(explanations_preds))\n",
    "            # print(convert_to_sentences(explanations_labels))\n",
    "            \n",
    "            # Ensure lengths match for accuracy calculation\n",
    "            if len(extracted_preds) == len(extracted_labels):\n",
    "                accuracy_preds.extend(extracted_preds)\n",
    "                accuracy_labels.extend(extracted_labels)\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = sum(p == l for p, l in zip(accuracy_preds, accuracy_labels))\n",
    "    accuracy = correct / len(accuracy_preds) if accuracy_preds else 0.0\n",
    "    \n",
    "    \n",
    "    # Calculate BERTScore for explanations (explanations_preds and explanations_labels)\n",
    "    P_exp, R_exp, F1_exp = bert_score.score(convert_to_sentences(explanations_preds), convert_to_sentences(explanations_labels), lang=\"en\", rescale_with_baseline=True)\n",
    "    bertscore_exp_avg = F1_exp.mean().item()\n",
    "\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    # return {'val_loss': loss_value, 'accuracy': accuracy}\n",
    "    # return {'val_loss': loss_value, 'bertscore_exp': bertscore_exp_avg}\n",
    "    return {'val_loss': loss_value, 'accuracy': accuracy, 'bertscore_exp': bertscore_exp_avg}\n",
    "\n",
    "\n",
    "# # Training and validation\n",
    "# num_epochs = 1\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_metrics = train_loop(model, train_loader, optimizer)\n",
    "#     val_metrics = validate_loop(model, val_loader)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     print(f\"Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "#     print(f\"Validation Loss: {val_metrics['val_loss']:.4f}\")\n",
    "#     print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    \n",
    "# Training and validation\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    train_metrics = train_loop(model, train_loader, optimizer)\n",
    "    val_metrics = validate_loop(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "    print(f\"Validation Loss: {val_metrics['val_loss']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Validation BERTScore (Explanations): {val_metrics['bertscore_exp']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

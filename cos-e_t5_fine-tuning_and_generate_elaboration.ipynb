{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COSE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the CoS-E dataset\n",
    "dataset = load_dataset('cos_e', 'v1.11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example\n",
    "example = dataset['train']\n",
    "print(f\"Question: {example['question'][0]}\")\n",
    "print(example['choices'][0])\n",
    "print(f\"Answer: {example['answer'][0]}\")\n",
    "print(f\"Explanation: {example['abstractive_explanation'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CoS-E dataset\n",
    "dataset = load_dataset('cos_e', 'v1.11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"  # Use \"t5-base\" or \"t5-large\" if resources allow\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q, choices in zip(examples['question'], examples['choices']):\n",
    "    print(q)\n",
    "    print(choices)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[\"question: \" + q + \" answer: \" + \" \".join(choices) for q, choices in zip(examples['question'], examples['choices'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples['answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples['abstractive_explanation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"answer: \" + examples['answer'][0] + \" explanation: \" + examples['abstractive_explanation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [\"answer: \" + answer + \" explanation: \" + explanation for answer, explanation in zip(examples['answer'], examples['abstractive_explanation'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"question: \" + q + \" answer: \" + \" \".join(choices) for q, choices in zip(examples['question'], examples['choices'])]\n",
    "    targets = [\"answer: \" + answer + \" explanation: \" + explanation for answer, explanation in zip(examples['answer'], examples['abstractive_explanation'])]\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(targets, max_length=256, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset['validation']['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(encoded_dataset['validation']['labels'][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Convert dataset to PyTorch tensors\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(encoded_dataset['train'], batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(encoded_dataset['validation'], batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create DataLoaders\n",
    "# train_dataset = encoded_dataset['train']\n",
    "# val_dataset = encoded_dataset['validation']\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_loop(model, loader, optimizer, accumulation_steps=2):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(loader, desc='Training:')):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "        loss = outputs.loss / accumulation_steps  # normalize loss\n",
    "\n",
    "        batch_loss_value = loss.item() * accumulation_steps  # convert to original loss value for logging\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:  # update weights every accumulation_steps mini-batches\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # reset gradients\n",
    "\n",
    "        batch_losses.append(batch_loss_value)\n",
    "\n",
    "    # Update remaining gradients if the number of batches is not a multiple of accumulation_steps\n",
    "    if len(loader) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    return {'train_loss': loss_value}\n",
    "\n",
    "# # Validation loop\n",
    "# def validate_loop(model, loader):\n",
    "#     model.eval()\n",
    "#     batch_losses = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(loader, desc='Validation:'):\n",
    "#             inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "#             labels = batch['labels'].to(device)\n",
    "#             outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "#             loss = outputs.loss\n",
    "\n",
    "#             batch_losses.append(loss.item())\n",
    "\n",
    "#     loss_value = sum(batch_losses) / len(batch_losses)\n",
    "#     return {'val_loss': loss_value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_loop(model, loader):\n",
    "#     model.eval()\n",
    "#     batch_losses = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(loader, desc='Validation:'):\n",
    "#             inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "#             labels = batch['labels'].to(device)\n",
    "#             outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "#             loss = outputs.loss\n",
    "\n",
    "#             # Generate predictions\n",
    "#             predictions = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512)\n",
    "\n",
    "#             # Decode predictions\n",
    "#             decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#             decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#             # Print decoded predictions and labels for debugging\n",
    "#             print(f\"Decoded predictions: {decoded_preds}\")\n",
    "#             print(f\"Decoded labels: {decoded_labels}\")\n",
    "\n",
    "#             batch_losses.append(loss.item())\n",
    "\n",
    "#     loss_value = sum(batch_losses) / len(batch_losses)\n",
    "#     return {'val_loss': loss_value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add accuracy validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from datasets import load_metric\n",
    "\n",
    "# # Load metrics\n",
    "# accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "# def validate_loop(model, loader):\n",
    "#     model.eval()\n",
    "#     batch_losses = []\n",
    "#     accuracy_preds = []\n",
    "#     accuracy_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(loader, desc='Validation:'):\n",
    "#             inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "#             labels = batch['labels'].to(device)\n",
    "#             outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "#             loss = outputs.loss\n",
    "\n",
    "#             # Generate predictions\n",
    "#             predictions = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512)\n",
    "\n",
    "#             # Decode predictions and labels\n",
    "#             decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#             decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#             # Print decoded predictions and labels for debugging\n",
    "#             print(f\"Decoded predictions: {decoded_preds}\")\n",
    "#             print(f\"Decoded labels: {decoded_labels}\")\n",
    "\n",
    "#             # Extract the answers from decoded predictions and labels\n",
    "#             extracted_preds = [pred.split('answer: ')[1].split(' ')[0] for pred in decoded_preds if 'answer: ' in pred]\n",
    "#             extracted_labels = [label.split('answer: ')[1].split(' ')[0] for label in decoded_labels if 'answer: ' in label]\n",
    "#             print(f\"extracted_preds: {extracted_preds}\")\n",
    "#             print(f\"extracted_labels:  {extracted_labels}\")\n",
    "#             # Ensure lengths match for accuracy calculation\n",
    "#             if len(extracted_preds) == len(extracted_labels):\n",
    "#                 accuracy_preds.extend(extracted_preds)\n",
    "#                 accuracy_labels.extend(extracted_labels)\n",
    "\n",
    "#             batch_losses.append(loss.item())\n",
    "\n",
    "#     # Calculate accuracy\n",
    "#     accuracy = accuracy_metric.compute(predictions=accuracy_preds, references=accuracy_labels)\n",
    "#     loss_value = sum(batch_losses) / len(batch_losses)\n",
    "\n",
    "#     return {'val_loss': loss_value, 'accuracy': accuracy['accuracy']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add accuracy version 2(customized acc metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_loop(model, loader):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    accuracy_preds = []\n",
    "    accuracy_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Validation:'):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Generate predictions\n",
    "            predictions = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512)\n",
    "\n",
    "            # Decode predictions and labels\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Print decoded predictions and labels for debugging\n",
    "            # print(\"Decoded predictions:\")\n",
    "            # for pred in decoded_preds:\n",
    "            #     print(f\"'{pred}'\")\n",
    "            # print(\"Decoded labels:\")\n",
    "            # for label in decoded_labels:\n",
    "            #     print(f\"'{label}'\")\n",
    "\n",
    "            # # Extract the answers from decoded predictions and labels\n",
    "            # extracted_preds = [pred.strip().split('.')[0] for pred in decoded_preds if pred.strip()]\n",
    "            # extracted_labels = [label.strip().split('.')[0] for label in decoded_labels if label.strip()]\n",
    "            # Extract the answers from decoded predictions and labels\n",
    "            extracted_preds = [pred.split('answer: ')[1].split(' ')[0] for pred in decoded_preds if 'answer: ' in pred]\n",
    "            extracted_labels = [label.split('answer: ')[1].split(' ')[0] for label in decoded_labels if 'answer: ' in label]\n",
    "            # Ensure lengths match for accuracy calculation\n",
    "            if len(extracted_preds) == len(extracted_labels):\n",
    "                accuracy_preds.extend(extracted_preds)\n",
    "                accuracy_labels.extend(extracted_labels)\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = sum(p == l for p, l in zip(accuracy_preds, accuracy_labels))\n",
    "    accuracy = correct / len(accuracy_preds) if accuracy_preds else 0.0\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "\n",
    "    return {'val_loss': loss_value, 'accuracy': accuracy}\n",
    "\n",
    "# Training and validation\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    train_metrics = train_loop(model, train_loader, optimizer)\n",
    "    val_metrics = validate_loop(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "    print(f\"Validation Loss: {val_metrics['val_loss']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add bert score for explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bert_score\n",
    "# from datasets import load_metric\n",
    "\n",
    "# # Load metrics\n",
    "# accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "# # Training loop\n",
    "# def train_loop(model, loader, optimizer, accumulation_steps=16):\n",
    "#     model.train()\n",
    "#     batch_losses = []\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     for i, batch in enumerate(tqdm(loader, desc='Training:')):\n",
    "#         inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "#         loss = outputs.loss / accumulation_steps  # normalize loss\n",
    "\n",
    "#         batch_loss_value = loss.item() * accumulation_steps  # convert to original loss value for logging\n",
    "#         loss.backward()\n",
    "\n",
    "#         if (i + 1) % accumulation_steps == 0:  # update weights every accumulation_steps mini-batches\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()  # reset gradients\n",
    "\n",
    "#         batch_losses.append(batch_loss_value)\n",
    "\n",
    "#     # Update remaining gradients if the number of batches is not a multiple of accumulation_steps\n",
    "#     if len(loader) % accumulation_steps != 0:\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#     loss_value = sum(batch_losses) / len(batch_losses)\n",
    "#     return {'train_loss': loss_value}\n",
    "\n",
    "# # Validation loop\n",
    "# def validate_loop(model, loader):\n",
    "#     model.eval()\n",
    "#     batch_losses = []\n",
    "#     accuracy_preds = []\n",
    "#     accuracy_labels = []\n",
    "#     explanations_preds = []\n",
    "#     explanations_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(loader, desc='Validation:'):\n",
    "#             inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "#             labels = batch['labels'].to(device)\n",
    "#             outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "#             loss = outputs.loss\n",
    "\n",
    "#             # Generate predictions\n",
    "#             predictions = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512)\n",
    "\n",
    "#             # Decode predictions and labels\n",
    "#             decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#             decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#             # Print decoded predictions and labels for debugging\n",
    "#             print(\"Decoded predictions:\")\n",
    "#             for pred in decoded_preds:\n",
    "#                 print(f\"'{pred}'\")\n",
    "#             print(\"Decoded labels:\")\n",
    "#             for label in decoded_labels:\n",
    "#                 print(f\"'{label}'\")\n",
    "\n",
    "#             # Extract the answers from decoded predictions and labels\n",
    "#             # extracted_preds = [pred.strip().split('.')[0] for pred in decoded_preds if pred.strip()]\n",
    "#             # extracted_labels = [label.strip().split('.')[0] for label in decoded_labels if label.strip()]\n",
    "\n",
    "#             extracted_preds = [pred.split('answer: ')[1].split(' ')[0] for pred in decoded_preds if 'answer: ' in pred]\n",
    "#             extracted_labels = [label.split('answer: ')[1].split(' ')[0] for label in decoded_labels if 'answer: ' in label]\n",
    "            \n",
    "#             # Ensure lengths match for accuracy calculation\n",
    "#             if len(extracted_preds) == len(extracted_labels):\n",
    "#                 accuracy_preds.extend(extracted_preds)\n",
    "#                 accuracy_labels.extend(extracted_labels)\n",
    "\n",
    "#             # Collect explanations for BERTScore\n",
    "            \n",
    "#             # explanations_preds.extend(decoded_preds)\n",
    "#             # explanations_labels.extend(decoded_labels)\n",
    "#             explanations_preds = [pred.split('explanation: ')[1].split(' ')[0] for pred in decoded_preds if 'explanation: ' in pred]\n",
    "#             explanations_labels = [label.split('explanation: ')[1].split(' ')[0] for label in decoded_labels if 'explanation: ' in label]\n",
    "\n",
    "#             batch_losses.append(loss.item())\n",
    "\n",
    "#     # Calculate accuracy\n",
    "#     correct = sum(p == l for p, l in zip(accuracy_preds, accuracy_labels))\n",
    "#     accuracy = correct / len(accuracy_preds) if accuracy_preds else 0.0\n",
    "#     loss_value = sum(batch_losses) / len(batch_losses)\n",
    "\n",
    "#     # Calculate BERTScore\n",
    "#     P, R, F1 = bert_score.score(explanations_preds, explanations_labels, lang=\"en\", rescale_with_baseline=True)\n",
    "#     bertscore_avg = F1.mean().item()\n",
    "\n",
    "#     return {'val_loss': loss_value, 'accuracy': accuracy, 'bertscore': bertscore_avg}\n",
    "\n",
    "# # Training and validation\n",
    "# num_epochs = 3\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_metrics = train_loop(model, train_loader, optimizer)\n",
    "#     val_metrics = validate_loop(model, val_loader)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     print(f\"Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "#     print(f\"Validation Loss: {val_metrics['val_loss']:.4f}\")\n",
    "#     print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "#     print(f\"Validation BERTScore: {val_metrics['bertscore']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above code has problem,decode predictions has empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Validation loop with BERTScore for explanations and predictions\n",
    "# def validate_loop(model, loader):\n",
    "#     model.eval()\n",
    "#     batch_losses = []\n",
    "#     accuracy_preds = []\n",
    "#     accuracy_labels = []\n",
    "#     explanations_preds = []\n",
    "#     explanations_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(loader, desc='Validation:'):\n",
    "#             inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "#             labels = batch['labels'].to(device)\n",
    "#             outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "#             loss = outputs.loss\n",
    "\n",
    "#             # Generate predictions\n",
    "#             predictions = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512)\n",
    "\n",
    "#             # Decode predictions and labels\n",
    "#             decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#             decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#             # Print decoded predictions and labels for debugging\n",
    "#             print(\"Decoded predictions:\")\n",
    "#             for pred in decoded_preds:\n",
    "#                 print(f\"'{pred}'\")\n",
    "#             print(\"Decoded labels:\")\n",
    "#             for label in decoded_labels:\n",
    "#                 print(f\"'{label}'\")\n",
    "\n",
    "#             # Extract the answers from decoded predictions and labels\n",
    "#             extracted_preds = [pred.split('answer: ')[1].split(' ')[0] for pred in decoded_preds if 'answer: ' in pred]\n",
    "#             extracted_labels = [label.split('answer: ')[1].split(' ')[0] for label in decoded_labels if 'answer: ' in label]\n",
    "\n",
    "#             # Ensure lengths match for accuracy calculation\n",
    "#             if len(extracted_preds) == len(extracted_labels):\n",
    "#                 accuracy_preds.extend(extracted_preds)\n",
    "#                 accuracy_labels.extend(extracted_labels)\n",
    "\n",
    "#             # Collect explanations for BERTScore\n",
    "#             # explanations_preds.extend(decoded_preds)\n",
    "#             # explanations_labels.extend(decoded_labels)\n",
    "#             explanations_preds = [pred.split('explanation: ')[1].split(' ')[0] for pred in decoded_preds if 'explanation: ' in pred]\n",
    "#             explanations_labels = [label.split('explanation: ')[1].split(' ')[0] for label in decoded_labels if 'explanation: ' in label]\n",
    "\n",
    "#             batch_losses.append(loss.item())\n",
    "\n",
    "#     # Calculate BERTScore for answers (accuracy_preds and accuracy_labels)\n",
    "#     P_ans, R_ans, F1_ans = bert_score.score(accuracy_preds, accuracy_labels, lang=\"en\", rescale_with_baseline=True)\n",
    "#     bertscore_ans_avg = F1_ans.mean().item()\n",
    "\n",
    "#     # Calculate BERTScore for explanations (explanations_preds and explanations_labels)\n",
    "#     P_exp, R_exp, F1_exp = bert_score.score(explanations_preds, explanations_labels, lang=\"en\", rescale_with_baseline=True)\n",
    "#     bertscore_exp_avg = F1_exp.mean().item()\n",
    "\n",
    "#     loss_value = sum(batch_losses) / len(batch_losses)\n",
    "\n",
    "#     return {'val_loss': loss_value, 'bertscore_ans': bertscore_ans_avg, 'bertscore_exp': bertscore_exp_avg}\n",
    "\n",
    "# # Training and validation\n",
    "# num_epochs = 3\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_metrics = train_loop(model, train_loader, optimizer)\n",
    "#     val_metrics = validate_loop(model, val_loader)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     print(f\"Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "#     print(f\"Validation Loss: {val_metrics['val_loss']:.4f}\")\n",
    "#     print(f\"Validation BERTScore (Answers): {val_metrics['bertscore_ans']:.4f}\")\n",
    "#     print(f\"Validation BERTScore (Explanations): {val_metrics['bertscore_exp']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is empty sentences in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sentences(list_of_lists):\n",
    "    sentences = [' '.join(inner_list) for inner_list in list_of_lists]\n",
    "    return sentences\n",
    "\n",
    "# Example usage\n",
    "list_of_lists = [\n",
    "    [\"The\", \"capital\", \"of\", \"France\", \"is\", \"Paris.\"],\n",
    "    [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog.\"],\n",
    "    [\"Artificial\", \"intelligence\", \"is\", \"transforming\", \"the\", \"world.\"]\n",
    "]\n",
    "\n",
    "sentences = convert_to_sentences(list_of_lists)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score\n",
    "from datasets import load_metric\n",
    "def validate_loop(model, loader):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    accuracy_preds = []\n",
    "    accuracy_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Validation:'):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Generate predictions\n",
    "            predictions = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512)\n",
    "\n",
    "            # Decode predictions and labels\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Print decoded predictions and labels for debugging\n",
    "            # print(\"Decoded predictions:\")\n",
    "            # for pred in decoded_preds:\n",
    "            #     print(f\"'{pred}'\")\n",
    "            # print(\"Decoded labels:\")\n",
    "            # for label in decoded_labels:\n",
    "            #     print(f\"'{label}'\")\n",
    "\n",
    "            # # Extract the answers from decoded predictions and labels\n",
    "            # extracted_preds = [pred.strip().split('.')[0] for pred in decoded_preds if pred.strip()]\n",
    "            # extracted_labels = [label.strip().split('.')[0] for label in decoded_labels if label.strip()]\n",
    "            # Extract the answers from decoded predictions and labels\n",
    "            extracted_preds = [pred.split('answer: ')[1].split(' ')[0] for pred in decoded_preds if 'answer: ' in pred]\n",
    "            extracted_labels = [label.split('answer: ')[1].split(' ')[0] for label in decoded_labels if 'answer: ' in label]\n",
    "            \n",
    "            explanations_preds = [pred.split('explanation: ')[1].split(' ') for pred in decoded_preds if 'explanation: ' in pred]\n",
    "            explanations_labels = [label.split('explanation: ')[1].split(' ') for label in decoded_labels if 'explanation: ' in label]\n",
    "            print(convert_to_sentences(explanations_preds))\n",
    "            print(convert_to_sentences(explanations_labels))\n",
    "            \n",
    "            # Ensure lengths match for accuracy calculation\n",
    "            if len(extracted_preds) == len(extracted_labels):\n",
    "                accuracy_preds.extend(extracted_preds)\n",
    "                accuracy_labels.extend(extracted_labels)\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = sum(p == l for p, l in zip(accuracy_preds, accuracy_labels))\n",
    "    accuracy = correct / len(accuracy_preds) if accuracy_preds else 0.0\n",
    "    \n",
    "    \n",
    "    # Calculate BERTScore for explanations (explanations_preds and explanations_labels)\n",
    "    P_exp, R_exp, F1_exp,_= bert_score.score(convert_to_sentences(explanations_preds), convert_to_sentences(explanations_labels), lang=\"en\", rescale_with_baseline=True)\n",
    "    bertscore_exp_avg = F1_exp.mean().item()\n",
    "\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    # return {'val_loss': loss_value, 'accuracy': accuracy}\n",
    "    # return {'val_loss': loss_value, 'bertscore_exp': bertscore_exp_avg}\n",
    "    return {'val_loss': loss_value, 'accuracy': accuracy, 'bertscore_exp': bertscore_exp_avg}\n",
    "\n",
    "\n",
    "# # Training and validation\n",
    "# num_epochs = 1\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_metrics = train_loop(model, train_loader, optimizer)\n",
    "#     val_metrics = validate_loop(model, val_loader)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     print(f\"Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "#     print(f\"Validation Loss: {val_metrics['val_loss']:.4f}\")\n",
    "#     print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    \n",
    "# Training and validation\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    train_metrics = train_loop(model, train_loader, optimizer)\n",
    "    val_metrics = validate_loop(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "    print(f\"Validation Loss: {val_metrics['val_loss']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Validation BERTScore (Explanations): {val_metrics['bertscore_exp']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_sentences(list_of_lists):\n",
    "#     sentences = [' '.join(inner_list) for inner_list in list_of_lists]\n",
    "#     return sentences\n",
    "\n",
    "# # Example usage\n",
    "# list_of_lists = [\n",
    "#     [\"The\", \"capital\", \"of\", \"France\", \"is\", \"Paris.\"],\n",
    "#     [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog.\"],\n",
    "#     [\"Artificial\", \"intelligence\", \"is\", \"transforming\", \"the\", \"world.\"]\n",
    "# ]\n",
    "\n",
    "# sentences = convert_to_sentences(list_of_lists)\n",
    "# print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase2 <br>\n",
    "Adding dream to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/DREAM\") # can take a couple minutes, be patient!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-11b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: \"There are 10 apples on an apple tree.  Three fall off.  Now there are X apples.\"  What is this an example of?\n",
    "['park', 'coloring book', 'garden center', 'math problem', 'gravity']\n",
    "Answer: math problem\n",
    "Explanation: webmath is designed to help you solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"$answer$ ; $question$ = [SITUATION] There are 10 apples on an apple tree.  Three fall off.  Now there are X apples. What is this an example of?  math or gravity?. [QUERY] consequence\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"$answer$ ; $question$ = [SITUATION] There are 10 apples on an apple tree.  Three fall off.  Now there are X apples. What is this an example of? math or gravity?.  [QUERY] rot\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "# [\"$answer$ = It's wrong to damage other people's property.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create elaboration from dream model in two ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['validation']['question'][0])\n",
    "print(dataset['validation']['choices'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with or of choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question(example):\n",
    "    situation = example['question']\n",
    "    choices = \" or \".join(example['choices'])\n",
    "    input_string = f\"$answer$ ; $question$ = [SITUATION] {situation}  {choices} [QUERY] rot\"\n",
    "    example['formatted_question'] = input_string\n",
    "    return example\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "formatted_dataset = dataset.map(format_question)\n",
    "\n",
    "# Print a few examples to verify the formatting\n",
    "print(formatted_dataset['train'][0]['formatted_question'])\n",
    "print(formatted_dataset['validation'][0]['formatted_question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_string = \"$answer$ ; $question$ = [SITUATION] There are 10 apples on an apple tree.  Three fall off.  Now there are X apples. What is this an example of? math or gravity?.  [QUERY] rot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rot dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply dream to formatted_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(example):\n",
    "    input_string = example['formatted_question']\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(input_ids, max_length=100)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    example['generated_output'] = generated_text\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=formatted_dataset['validation'][2]\n",
    "input_string = example['formatted_question']\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(input_ids, max_length=100)\n",
    "print(output)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "example['generated_output'] = generated_text\n",
    "print(example['generated_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example['formatted_question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to generate outputs\n",
    "result_dataset = formatted_dataset.map(generate_output)\n",
    "\n",
    "# Print a few examples to verify the outputs\n",
    "print(result_dataset['train'][0]['generated_output'])\n",
    "print(result_dataset['validation'][0]['generated_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to save to a directory named \"formatted_dataset\"\n",
    "result_dataset.save_to_disk('./formatted_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add dream model text to input of t5 and fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debugging not important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**debugging phase and bleurt score that doesn't work because of google api not work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def validation loop using bert score for explanation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets transformers torch tqdm bert-score bleurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip  # ensures that pip is current\n",
    "# !git clone https://github.com/google-research/bleurt.git\n",
    "# !pip install ./bleurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /home/manavi/bleurt-base-128.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleurt_metric = load(\"bleurt\", \"/home/manavi/bleurt-base-128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "# accuracy_metric = load_metric(\"accuracy\")\n",
    "# bert_score_metric = load_metric(\"bertscore\")\n",
    "# bleurt_metric = load_metric(\"bleurt\")\n",
    "\n",
    "# accuracy_metric = load_metric(\"accuracy\")\n",
    "# bert_score_metric = load_metric(\"bertscore\", trust_remote_code=True)\n",
    "# bleurt_metric = load_metric(\"bleurt\",trust_remote_code=True, checkpoint=\"bleurt-base-128\")\n",
    "\n",
    "# Load metrics\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "bert_score_metric = load_metric(\"bertscore\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_loop(model, loader, optimizer, accumulation_steps=16):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(loader, desc='Training:')):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "        loss = outputs.loss / accumulation_steps  # normalize loss\n",
    "\n",
    "        batch_loss_value = loss.item() * accumulation_steps  # convert to original loss value for logging\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:  # update weights every accumulation_steps mini-batches\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # reset gradients\n",
    "\n",
    "        batch_losses.append(batch_loss_value)\n",
    "\n",
    "    # Update remaining gradients if the number of batches is not a multiple of accumulation_steps\n",
    "    if len(loader) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    return {'train_loss': loss_value}\n",
    "\n",
    "# Validation loop\n",
    "def validate_loop(model, loader):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    accuracy_preds = []\n",
    "    accuracy_labels = []\n",
    "    bertscore_preds = []\n",
    "    bertscore_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Validation:'):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Generate predictions\n",
    "            predictions = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Prepare for accuracy metric\n",
    "            accuracy_preds.extend(decoded_preds)\n",
    "            accuracy_labels.extend(decoded_labels)\n",
    "\n",
    "            # Prepare for BERTScore metric\n",
    "            bertscore_preds.extend(decoded_preds)\n",
    "            bertscore_labels.extend(decoded_labels)\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "    # Compute metrics\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    accuracy = accuracy_metric.compute(predictions=accuracy_preds, references=accuracy_labels)\n",
    "    bert_score = bert_score_metric.compute(predictions=bertscore_preds, references=bertscore_labels, lang=\"en\")\n",
    "    bert_score_avg = sum(bert_score['f1']) / len(bert_score['f1'])\n",
    "\n",
    "    return {\n",
    "        'val_loss': loss_value,\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'bert_score': bert_score_avg\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_loop(model, loader, optimizer, accumulation_steps=16):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(loader, desc='Training:')):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "        loss = outputs.loss / accumulation_steps  # normalize loss\n",
    "\n",
    "        batch_loss_value = loss.item() * accumulation_steps  # convert to original loss value for logging\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:  # update weights every accumulation_steps mini-batches\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # reset gradients\n",
    "\n",
    "        batch_losses.append(batch_loss_value)\n",
    "\n",
    "    # Update remaining gradients if the number of batches is not a multiple of accumulation_steps\n",
    "    if len(loader) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    return {'train_loss': loss_value}\n",
    "\n",
    "# Validation loop\n",
    "def validate_loop(model, loader):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    accuracy_preds = []\n",
    "    accuracy_labels = []\n",
    "    bertscore_preds = []\n",
    "    bertscore_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Validation:'):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Generate predictions\n",
    "            predictions = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Filter out empty strings\n",
    "            decoded_preds = [pred for pred in decoded_preds if pred.strip()]\n",
    "            decoded_labels = [label for label in decoded_labels if label.strip()]\n",
    "\n",
    "            # Prepare for accuracy metric (assuming labels are in the form of text that needs to be converted to integers)\n",
    "            accuracy_preds.extend([int(pred.split()[1]) for pred in decoded_preds if pred.split()[1].isdigit()])\n",
    "            accuracy_labels.extend([int(label.split()[1]) for label in decoded_labels if label.split()[1].isdigit()])\n",
    "\n",
    "            # Prepare for BERTScore metric\n",
    "            bertscore_preds.extend(decoded_preds)\n",
    "            bertscore_labels.extend(decoded_labels)\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "    # Compute metrics\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    accuracy = accuracy_metric.compute(predictions=accuracy_preds, references=accuracy_labels)\n",
    "    bert_score = bert_score_metric.compute(predictions=bertscore_preds, references=bertscore_labels, lang=\"en\")\n",
    "    bert_score_avg = sum(bert_score['f1']) / len(bert_score['f1'])\n",
    "\n",
    "    return {\n",
    "        'val_loss': loss_value,\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'bert_score': bert_score_avg\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/google-research/bleurt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/google-research/bleurt.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    train_metrics = train_loop(model, train_loader, optimizer)\n",
    "    val_metrics = validate_loop(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "    print(f\"Validation Loss: {val_metrics['val_loss']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Explanation Score: {val_metrics['explanation_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_loop(model, loader, optimizer, accumulation_steps=16):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(loader, desc='Training:')):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "        loss = outputs.loss / accumulation_steps  # normalize loss\n",
    "\n",
    "        batch_loss_value = loss.item() * accumulation_steps  # convert to original loss value for logging\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:  # update weights every accumulation_steps mini-batches\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # reset gradients\n",
    "\n",
    "        batch_losses.append(batch_loss_value)\n",
    "\n",
    "    # Update remaining gradients if the number of batches is not a multiple of accumulation_steps\n",
    "    if len(loader) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    return {'train_loss': loss_value}\n",
    "\n",
    "# Validation loop\n",
    "def validate_loop(model, loader):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    accuracy_preds = []\n",
    "    accuracy_labels = []\n",
    "    bertscore_preds = []\n",
    "    bertscore_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Validation:'):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Generate predictions\n",
    "            predictions = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Filter out empty strings and check lengths\n",
    "            decoded_preds = [pred for pred in decoded_preds if pred.strip()]\n",
    "            decoded_labels = [label for label in decoded_labels if label.strip()]\n",
    "\n",
    "            # Debugging output\n",
    "            print(f\"Decoded predictions: {decoded_preds}\")\n",
    "            print(f\"Decoded labels: {decoded_labels}\")\n",
    "\n",
    "            # Prepare for accuracy metric (assuming labels are in the form of text that needs to be converted to integers)\n",
    "            accuracy_preds.extend([int(pred.split()[1]) for pred in decoded_preds if pred.split()[1].isdigit()])\n",
    "            accuracy_labels.extend([int(label.split()[1]) for label in decoded_labels if label.split()[1].isdigit()])\n",
    "\n",
    "            # Prepare for BERTScore metric\n",
    "            bertscore_preds.extend(decoded_preds)\n",
    "            bertscore_labels.extend(decoded_labels)\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "    # Ensure no empty lists before computing metrics\n",
    "    if not accuracy_preds or not accuracy_labels:\n",
    "        raise ValueError(\"Empty list found for accuracy predictions or labels.\")\n",
    "    if not bertscore_preds or not bertscore_labels:\n",
    "        raise ValueError(\"Empty list found for BERTScore predictions or labels.\")\n",
    "\n",
    "    # Compute metrics\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    accuracy = accuracy_metric.compute(predictions=accuracy_preds, references=accuracy_labels)\n",
    "    bert_score = bert_score_metric.compute(predictions=bertscore_preds, references=bertscore_labels, lang=\"en\")\n",
    "    bert_score_avg = sum(bert_score['f1']) / len(bert_score['f1'])\n",
    "\n",
    "    return {\n",
    "        'val_loss': loss_value,\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'bert_score': bert_score_avg\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    train_metrics = train_loop(model, train_loader, optimizer)\n",
    "    val_metrics = validate_loop(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "    print(f\"Validation Loss: {val_metrics['val_loss']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Explanation Score: {val_metrics['explanation_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_loop(model, loader, optimizer, accumulation_steps=16):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(loader, desc='Training:')):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "        loss = outputs.loss / accumulation_steps  # normalize loss\n",
    "\n",
    "        batch_loss_value = loss.item() * accumulation_steps  # convert to original loss value for logging\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:  # update weights every accumulation_steps mini-batches\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # reset gradients\n",
    "\n",
    "        batch_losses.append(batch_loss_value)\n",
    "\n",
    "    # Update remaining gradients if the number of batches is not a multiple of accumulation_steps\n",
    "    if len(loader) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    return {'train_loss': loss_value}\n",
    "\n",
    "# Validation loop\n",
    "def validate_loop(model, loader):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    accuracy_preds = []\n",
    "    accuracy_labels = []\n",
    "    bertscore_preds = []\n",
    "    bertscore_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Validation:'):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Generate predictions\n",
    "            predictions = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Debugging output\n",
    "            print(f\"Decoded predictions: {decoded_preds}\")\n",
    "            print(f\"Decoded labels: {decoded_labels}\")\n",
    "\n",
    "            # Filter out empty strings and check lengths\n",
    "            decoded_preds = [pred for pred in decoded_preds if pred.strip()]\n",
    "            decoded_labels = [label for label in decoded_labels if label.strip()]\n",
    "\n",
    "            # Ensure lengths match for accuracy metric\n",
    "            if len(decoded_preds) != len(decoded_labels):\n",
    "                print(f\"Length mismatch: {len(decoded_preds)} predictions, {len(decoded_labels)} labels\")\n",
    "                continue\n",
    "\n",
    "            # Prepare for accuracy metric (assuming labels are in the form of text that needs to be converted to integers)\n",
    "            try:\n",
    "                accuracy_preds.extend([int(pred.split()[1]) for pred in decoded_preds if len(pred.split()) > 1 and pred.split()[1].isdigit()])\n",
    "                accuracy_labels.extend([int(label.split()[1]) for label in decoded_labels if len(label.split()) > 1 and label.split()[1].isdigit()])\n",
    "            except IndexError as e:\n",
    "                print(f\"Index error: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Prepare for BERTScore metric\n",
    "            bertscore_preds.extend(decoded_preds)\n",
    "            bertscore_labels.extend(decoded_labels)\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "    # Ensure no empty lists before computing metrics\n",
    "    if not accuracy_preds or not accuracy_labels:\n",
    "        print(f\"Accuracy preds: {accuracy_preds}\")\n",
    "        print(f\"Accuracy labels: {accuracy_labels}\")\n",
    "        raise ValueError(\"Empty list found for accuracy predictions or labels.\")\n",
    "    if not bertscore_preds or not bertscore_labels:\n",
    "        print(f\"BERTScore preds: {bertscore_preds}\")\n",
    "        print(f\"BERTScore labels: {bertscore_labels}\")\n",
    "        raise ValueError(\"Empty list found for BERTScore predictions or labels.\")\n",
    "\n",
    "    # Compute metrics\n",
    "    loss_value = sum(batch_losses) / len(batch_losses)\n",
    "    accuracy = accuracy_metric.compute(predictions=accuracy_preds, references=accuracy_labels)\n",
    "    bert_score = bert_score_metric.compute(predictions=bertscore_preds, references=bertscore_labels, lang=\"en\")\n",
    "    bert_score_avg = sum(bert_score['f1']) / len(bert_score['f1'])\n",
    "\n",
    "    return {\n",
    "        'val_loss': loss_value,\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'bert_score': bert_score_avg\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    train_metrics = train_loop(model, train_loader, optimizer)\n",
    "    val_metrics = validate_loop(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "    print(f\"Validation Loss: {val_metrics['val_loss']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Validation BERTScore: {val_metrics['bert_score']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
